{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "# Assume that we are on a CUDA machine, then this should print a CUDA device:\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace with any text file containing full set of data\n",
    "mozart_data = 'txt-files/notewise/mozart.txt'\n",
    "\n",
    "with open(mozart_data, 'r') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this part needs only when the volume includes\n",
    "text_list = text.split()\n",
    "a = ['5p2','5endp2','5p1','5endp1','5p0','5endp0']\n",
    "text_list.extend(a)\n",
    "# get vocabulary set\n",
    "words = sorted(tuple(set(text_list)))\n",
    "n = len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get vocabulary set\n",
    "words = sorted(tuple(set(text.split())))\n",
    "n = len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create word-integer encoder/decoder\n",
    "word2int = dict(zip(words, list(range(n))))\n",
    "int2word = dict(zip(list(range(n)), words))\n",
    "\n",
    "# encode all words in dataset into integers\n",
    "encoded = np.array([word2int[word] for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model using the pytorch nn module\n",
    "class WordLSTM(nn.ModuleList):\n",
    "    \n",
    "    def __init__(self, sequence_len, vocab_size, hidden_dim, batch_size):\n",
    "        super(WordLSTM, self).__init__()\n",
    "        \n",
    "        # init the hyperparameters\n",
    "        self.vocab_size = vocab_size\n",
    "        self.sequence_len = sequence_len\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # first layer lstm cell\n",
    "        self.lstm_1 = nn.LSTMCell(input_size=vocab_size, hidden_size=hidden_dim)\n",
    "        \n",
    "        # second layer lstm cell\n",
    "        self.lstm_2 = nn.LSTMCell(input_size=hidden_dim, hidden_size=hidden_dim)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "        # fully connected layer\n",
    "        self.fc = nn.Linear(in_features=hidden_dim, out_features=vocab_size)\n",
    "        \n",
    "    # forward pass in training   \n",
    "    def forward(self, x, hc):\n",
    "        \"\"\"\n",
    "            accepts 2 arguments: \n",
    "            1. x: input of each batch \n",
    "                - shape 128*149 (batch_size*vocab_size)\n",
    "            2. hc: tuple of init hidden, cell states \n",
    "                - each of shape 128*512 (batch_size*hidden_dim)\n",
    "        \"\"\"\n",
    "        \n",
    "        # create empty output seq\n",
    "        output_seq = torch.empty((self.sequence_len,\n",
    "                                  self.batch_size,\n",
    "                                  self.vocab_size))\n",
    "        # if using gpu        \n",
    "        output_seq = output_seq.to(device)\n",
    "        \n",
    "        # init hidden, cell states for lstm layers\n",
    "        hc_1, hc_2 = hc, hc\n",
    "        \n",
    "        # for t-th word in every sequence \n",
    "        for t in range(self.sequence_len):\n",
    "            \n",
    "            # layer 1 lstm\n",
    "            hc_1 = self.lstm_1(x[t], hc_1)\n",
    "            h_1, c_1 = hc_1\n",
    "            \n",
    "            # layer 2 lstm\n",
    "            hc_2 = self.lstm_2(h_1, hc_2)\n",
    "            h_2, c_2 = hc_2\n",
    "            \n",
    "            # dropout and fully connected layer\n",
    "            output_seq[t] = self.fc(self.dropout(h_2))\n",
    "            \n",
    "        return output_seq.view((self.sequence_len * self.batch_size, -1))\n",
    "          \n",
    "    def init_hidden(self):\n",
    "        \n",
    "        # initialize hidden, cell states for training\n",
    "        # if using gpu\n",
    "        return (torch.zeros(self.batch_size, self.hidden_dim).to(device),\n",
    "                torch.zeros(self.batch_size, self.hidden_dim).to(device))\n",
    "    \n",
    "    def init_hidden_generator(self):\n",
    "        \n",
    "        # initialize hidden, cell states for prediction of 1 sequence\n",
    "        # if using gpu\n",
    "        return (torch.zeros(1, self.hidden_dim).to(device),\n",
    "                torch.zeros(1, self.hidden_dim).to(device))\n",
    "    \n",
    "    def predict(self, seed_seq, top_k=5, pred_len=128):\n",
    "        \"\"\"\n",
    "            accepts 3 arguments: \n",
    "            1. seed_seq: seed string sequence for prediction (prompt)\n",
    "            2. top_k: top k words to sample prediction from\n",
    "            3. pred_len: number of words to generate after the seed seq\n",
    "        \"\"\"\n",
    "        \n",
    "        # set evaluation mode\n",
    "        self.eval()\n",
    "        \n",
    "        # split string into list of words\n",
    "        seed_seq = seed_seq.split()\n",
    "        \n",
    "        # get seed sequence length\n",
    "        seed_len = len(seed_seq)\n",
    "        \n",
    "        # create output sequence\n",
    "        out_seq = np.empty(seed_len+pred_len)\n",
    "        \n",
    "        # append input seq to output seq\n",
    "        out_seq[:seed_len] = np.array([word2int[word] for word in seed_seq])\n",
    " \n",
    "        # init hidden, cell states for generation\n",
    "        hc = self.init_hidden_generator()\n",
    "        hc_1, hc_2 = hc, hc\n",
    "        \n",
    "        # feed seed string into lstm\n",
    "        # get the hidden state set up\n",
    "        for word in seed_seq[:-1]:\n",
    "            \n",
    "            # encode starting word to one-hot encoding\n",
    "            word = to_categorical(word2int[word], num_classes=self.vocab_size)\n",
    "\n",
    "            # add batch dimension\n",
    "            word = torch.from_numpy(word).unsqueeze(0)\n",
    "            # if using gpu\n",
    "            word = word.to(device) \n",
    "            \n",
    "            # layer 1 lstm\n",
    "            hc_1 = self.lstm_1(word, hc_1)\n",
    "            h_1, c_1 = hc_1\n",
    "            \n",
    "            # layer 2 lstm\n",
    "            hc_2 = self.lstm_2(h_1, hc_2)\n",
    "            h_2, c_2 = hc_2\n",
    "        \n",
    "        word = seed_seq[-1]\n",
    "        \n",
    "        # encode starting word to one-hot encoding\n",
    "        word = to_categorical(word2int[word], num_classes=self.vocab_size)\n",
    "\n",
    "        # add batch dimension\n",
    "        word = torch.from_numpy(word).unsqueeze(0)\n",
    "        # if using gpu\n",
    "        word = word.to(device) \n",
    "\n",
    "        # forward pass\n",
    "        for t in range(pred_len):\n",
    "            \n",
    "            # layer 1 lstm\n",
    "            hc_1 = self.lstm_1(word, hc_1)\n",
    "            h_1, c_1 = hc_1\n",
    "            \n",
    "            # layer 2 lstm\n",
    "            hc_2 = self.lstm_2(h_1, hc_2)\n",
    "            h_2, c_2 = hc_2\n",
    "            \n",
    "            # fully connected layer without dropout (no need)\n",
    "            output = self.fc(h_2)\n",
    "            \n",
    "            # software to get probabilities of output options\n",
    "            output = F.softmax(output, dim=1)\n",
    "            \n",
    "            # get top k words and corresponding probabilities\n",
    "            p, top_word = output.topk(top_k)\n",
    "            # if using gpu           \n",
    "            p = p.cpu()\n",
    "            \n",
    "            # sample from top k words to get next word\n",
    "            p = p.detach().squeeze().numpy()\n",
    "            top_word = torch.squeeze(top_word)\n",
    "            \n",
    "            word = np.random.choice(top_word, p = p/p.sum())\n",
    "            \n",
    "            # add word to sequence\n",
    "            out_seq[seed_len+t] = word\n",
    "            \n",
    "            # encode predicted word to one-hot encoding for next step\n",
    "            word = to_categorical(word, num_classes=self.vocab_size)\n",
    "            word = torch.from_numpy(word).unsqueeze(0)\n",
    "            # if using gpu\n",
    "            word = word.to(device)\n",
    "            \n",
    "        return out_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, n_seqs, n_words):\n",
    "    \"\"\"\n",
    "        create generator object that returns batches of input (x) and target (y).\n",
    "        x of each batch has shape 128*128*149 (batch_size*seq_len*vocab_size).\n",
    "        \n",
    "        accepts 3 arguments:\n",
    "        1. arr: array of words from text data\n",
    "        2. n_seq: number of sequence in each batch (aka batch_size)\n",
    "        3. n_word: number of words in each sequence\n",
    "    \"\"\"\n",
    "    \n",
    "    # compute total elements / dimension of each batch\n",
    "    batch_total = n_seqs * n_words\n",
    "    \n",
    "    # compute total number of complete batches\n",
    "    n_batches = arr.size//batch_total\n",
    "    \n",
    "    # chop array at the last full batch\n",
    "    arr = arr[: n_batches* batch_total]\n",
    "    \n",
    "    # reshape array to matrix with rows = no. of seq in one batch\n",
    "    arr = arr.reshape((n_seqs, -1))\n",
    "    \n",
    "    # for each n_words in every row of the dataset\n",
    "    for n in range(0, arr.shape[1], n_words):\n",
    "        \n",
    "        # chop it vertically, to get the input sequences\n",
    "        x = arr[:, n:n+n_words]\n",
    "        \n",
    "        # init y - target with shape same as x\n",
    "        y = np.zeros_like(x)\n",
    "        \n",
    "        # targets obtained by shifting by one\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], x[:, n+n_words]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n",
    "        \n",
    "        # yield function is like return, but creates a generator object\n",
    "        yield x, y   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the network - sequence_len, vocab_size, hidden_dim, batch_size\n",
    "net = WordLSTM(sequence_len=128, vocab_size=len(word2int), hidden_dim=512, batch_size=128)\n",
    "# if using gpu\n",
    "net.to(device)\n",
    "\n",
    "# define the loss and the optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# split dataset into 90% train and 10% using index\n",
    "val_idx = int(len(encoded) * (1 - 0.1))\n",
    "train_data, val_data = encoded[:val_idx], encoded[val_idx:]\n",
    "\n",
    "# empty list for the validation losses\n",
    "val_losses = list()\n",
    "\n",
    "# empty list for the samples\n",
    "samples = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'models/run3/lstm20'\n",
    "\n",
    "model = torch.load(model_path, map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get seed sequence\n",
    "\n",
    "seed_path = 'txt-files/notewise/Requiem/Requiem_-_Mozart_-_8._Lacrimosa_Piano_Truc.txt'\n",
    "\n",
    "with open(seed_path, \"r\") as myfile:\n",
    "    seed_seq = myfile.read()\n",
    "    myfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p29 p32 wait6 p40 wait2 endp29 endp32 wait3 endp40 wait1 p41 wait5 endp41 wait1 p32 p36 wait6 p48 wait2 endp32 endp36 wait3 endp48 wait1 p49 wait5 endp49 wait1 p31 p34 wait6 p41 wait2 endp31 endp34 wait3 endp41 wait1 p40 wait6 p34 p40 wait6 p51 wait2 endp34 endp40 wait3 endp51 wait1 p49 wait5 endp49 wait1 p32 p41 wait6 p48 wait2 endp32 endp41 wait3 endp48 wait1 p53 wait5 endp53 wait1 p22 p31 wait6 p49 wait2 endp22 endp31 wait3 endp49 wait1 p46 wait5 endp46 wait1 p24 p29 wait6 p43 wait2 endp24 endp29 wait3 endp43 wait1 p44 wait5 endp44 wait1 p24 p34 wait6 p48 wait2 endp24 endp34 wait3 endp48 wait1 p40 wait5 endp40 wait1 p5 p17 p32 wait5 endp32 wait1 p40 wait2 endp5 endp17 wait3 endp40 wait1 p41 wait5 endp41 wait1 p8 p20 p36 wait5 endp36 wait1 p48 wait2 endp8 endp20 wait3 endp48 wait1 p49 wait5 endp49 wait1 p7 p19 p34 wait5 endp34 wait1 p41 wait2 endp7 endp19 wait3 endp41 wait1 p40 wait6 p0 p12 p34 p40 wait5 endp34 endp40 wait1 p49 wait2 endp0 endp12 wait3 endp49 wait1 p48 wait5 endp48 wait1 p5 p17 p32 p41 wait5 endp32 endp41 wait1 p40 wait2 endp5 endp17 wait3 endp40 wait1 p41 wait5 endp41 wait1 p8 p20 p36 wait5 endp36 wait1 p48 wait2 endp8 endp20 wait3 endp48 wait1 p49 wait5 endp49 wait1 p7 p19 p34 wait5 endp34 wait1 p41 wait2 endp7 endp19 wait3 endp41 wait1 p40 wait6 p0 p12 p34 p40 wait5 endp34 endp40 wait1 p49 wait2 endp0 endp12 wait3 endp49 wait1 p48 wait5 endp48 wait1 p5 p17 p20 wait5 endp20 wait1 p29 wait2 endp5 endp17 wait3 endp29 wait1 p24 wait6 p0 p12 p24 p28 wait5 endp24 endp28 wait1 p31 wait2 endp0 endp12 wait3 endp31 wait1 p24 wait6 p5 p17 p24 p29 wait5 endp24 endp29 wait1 p32 wait2 endp5 endp17 wait3 endp32 wait1 p29 wait5 endp29 wait1 p3 p15 p27 p31 wait5 endp27 endp31 wait1 p34 wait2 endp3 endp15 wait3 endp34 wait1 p27 wait6 p8 p20 p27 p32 wait5 endp27 endp32 wait1 p36 wait2 endp8 endp20 wait3 endp36 wait1 p32 wait5 endp32 wait1 p7 p19 p23 p26 wait5 endp23 endp26 wait1 p38 wait2 endp7 endp19 wait3 endp38 wait1 p31 wait6 p0 p12 p24 p31 wait5 endp24 endp31 wait1 p39 wait2 endp0 endp12 wait3 endp39 wait1 p36 wait5 endp36 wait1 p10 p26 p34 wait5 endp26 endp34 wait1 p41 wait2 endp10 wait3 endp41 wait1 p34 wait6 p3 p15 p27 p34 wait5 endp27 endp34 wait1 p42 wait2 endp3 endp15 wait3 endp42 wait1 p39 wait6 p13 p25 p34 p39 wait5 endp34 endp39 wait1 p43 wait2 endp13 endp25 wait3 endp43 wait1 p39 wait6 p12 p24 p32 p39 wait5 endp32 endp39 wait1 p44 wait2 endp12 endp24 wait3 endp44 wait1 p39 wait5 endp39 wait1 p15 p27 p36 p42 wait5 endp36 endp42 wait1 p45 wait2 endp15 endp27 wait3 endp45 wait1 p36 wait5 endp36 wait1 p14 p26 p34 p41 wait5 endp34 endp41 wait1 p46 wait2 endp14 endp26 wait3 endp46 wait1 p34 wait5 endp34 wait1 p13 p25 p41 p44 wait5 endp41 endp44 wait1 p47 wait2 endp13 endp25 wait3 endp47 wait1 p35 wait5 endp35 wait1 p12 p24 p36 p41 p44 wait5 endp36 endp41 endp44 wait1 p48 wait2 endp12 endp24 wait3 endp48 wait1 p36 wait6 p0 p12 p36 p40 p43 wait5 endp40 endp43 wait1 p36 wait2 endp0 endp12 wait3 endp36 wait1 p24 wait5 endp24 wait2 \n"
     ]
    }
   ],
   "source": [
    "print(seed_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Requiem/lacrimosa_finished_20epoch.txt\", \"w\") as outfile:\n",
    "    outfile.write(' '.join([int2word[int_] for int_ in model.predict(seed_seq, pred_len=256)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
